---------------Regression - predicts a real value output based on inputs----------------

also be used in classification
classification - discrete valued output (basically 1 or 0 / True or False) 

conventions
given a training set = set of variables
m = # of training examples
x's = input variables - features
y's = output variables - target
(x,y) = training example
(xi,yi) = ith training example (i=index into training set)

Training set
	v
	v
	v
Learning Algorithm (job of the learning algorithm is to output the function (hypothesis) )
	v
	v
	v
	
	H >>>> estimated Price
	^
	^
	^
Hypothesis (takes input for example given a size what will the estimated price be)
(h maps from x's to y's)	
(size / volume /etc.)


How do we represent H(hypothesis)?
hğ›©(x) = ğ›©0 + ğ›©1(x) shorthand: h(x) (hypothesis is a string line funciton in this case)
simple best fit function = linear regression with one variable

what are the ğ›©'s? ğ›©'s represent parameters 
our goal with linear regression is to come up with values for 
ğ›©1 and ğ›©0 that provids a best fit for the data
We will do this by minimizing ğ›©0 and ğ›©1 - this is accomplished
by minimized the squared difference between the output of the hypothesis and the actual value y
1/2m(from i to m)âˆ‘(hğ›©(xi)-yi)^2

this is what is called the Cost Function = J(ğ›©0,ğ›©1) = 1/2m(from i to m)âˆ‘(hğ›©(xi)-yi)^2
goal is to minimuze over J(ğ›©0,ğ›©1) 
this is referred to as squared error (most commonly used for regression problems)


Cost Function - How to fit the best possible straight line to your data


----------------Cost Function Intuition------------------
hypothesis:hğ›©(x) = ğ›©0+ğ›©1(x)
parameters:(ğ›©0,ğ›©1)
cost function:J(ğ›©0,ğ›©1) = 1/2m(from i to m)âˆ‘(hğ›©(xi)-yi)^2

Goal: minimize-J(ğ›©0,ğ›©1)

for the sake of explanation lets simplify our cost function. ğ›©0 = 0 (lets make our y-intercept = 0 so that for the sake of understanding we will focus on the slope of the hypothesis function)

hypothesis:
is a function of (x) while

Cost Function
is a function of (ğ›©1[the slope])

-------------------Gradient Descent--------------------

Gradient Descent= an equation for minimizing the cost function. Gradient is more general pupose can be used to minimize an arbitrary function. 
Let's say we have some function J(ğ›©0,...ğ›©n) and we want to minimize that function.
min J(ğ›©0,...ğ›©n)

The idea is to keep changing (ğ›©1,ğ›©1) to reduce J(ğ›©1,ğ›©1)
until you hopefully end up at a minimum

Gradient Descent Algorithm
repeat until convergence
ğ›©j := ğ›©j - Î± âˆ‚/âˆ‚(ğ›©j) J(ğ›©0,ğ›©1) 
when executing the algorithm ensure you simultaneously update ğ›©0, and ğ›©1




